# -*- coding: utf-8 -*-
"""1_2_generateDTM_v2 revised by Chilin 8 Sep##.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cfgd-Rvqg_ZJU6KUhEj0TiK2huQWwCwL
"""

# -*- coding: utf-8 -*-
"""
Chilin's hands-on practice: Text Preparation (DTM)
"""
import nltk
from nltk import FreqDist, SnowballStemmer
import string  #module for string manipulation
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')  #punkt is a sentence tokenizer, dividing text into list of sentences

nltk.download('reuters') #Reuters Corpus contains 10,788 news documents totaling 1.3 million words, classified into 90 topics
from nltk.corpus import reuters

!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora

# We'll use the reuters corpus in NLTK.
# The same steps of preprocessing can be done on documents read in from external files.

# How many files are there in the corpus?
# What are their categories? Single or multiple categories for one file?
print("No. of documents: ", len(nltk.corpus.reuters.fileids()))
cats = [ reuters.categories(f) for f in reuters.fileids() ]
cat_num = [ len(c) for c in cats ]
fd_num = FreqDist(cat_num)
fd_num.plot()

# Most files have single category, but some do have two or more categories.
fd_num

# How many documents are there in each category?
# FreqDist() can be used to find the answer, but we need to flatten the list of categories first.
cats_flat = [ c for l in cats for c in l ]
fd_cat = FreqDist(cats_flat) #tuple of topic categories
# how many categories in total based on the most common
print (len(fd_cat))
print(fd_cat.most_common())

# Let's pick two categories
grain = reuters.fileids('grain')  #returns the ids of files relating to grain and trade
trade = reuters.fileids('trade')

print(len(grain))
print(len(trade))

grain_raw = [ reuters.raw(f) for f in grain ] 
trade_raw = [ reuters.raw(f) for f in trade ] 

print(grain_raw)
print (trade_raw)

# Let's explore the data by visualizing the articles in each category using word cloud
# For convenience, let's define a function myPrep() to perform the preprocessing steps given a file (text string):
#   tokenization, case lowering, punctuation removal, stopword removal, numeric removal
#   stemming/lemmatization
stop = stopwords.words('english')+['would', 'could']
snowball = SnowballStemmer('english')

def myPrep(tt):
    toks = nltk.word_tokenize(tt.lower()) #changes words in toks to lower case
    toks = [ t for t in toks if t not in string.punctuation+"’“”" ] #adds words to toks if not found in punctuation
    toks = [ t for t in toks if t not in stop ] #add words if not part of stopwords
    toks = [ t for t in toks if not t.isnumeric() ] #adds words if not numeric
    toks = [ snowball.stem(t) for t in toks ] #stems the words and adds to toks
    return toks

print(myPrep("This is a test sentence."))  #tests the myPrep function using 5 words
print(myPrep(trade_raw[0]))  #applies the myPrep function to trade_raw list

# Preprocess each file in each category
grain_clean = [ myPrep(f) for f in grain_raw ]
trade_clean = [ myPrep(f) for f in trade_raw ]

# Flatten the list of lists for FreqDist; break the docs
grain_flat = [ c for l in grain_clean for c in l ]
trade_flat = [ c for l in trade_clean for c in l ]

fd_grain = FreqDist(grain_flat)
fd_trade = FreqDist(trade_flat)

# Generate word clouds for the two categories.
from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc_grain = WordCloud(background_color="white").generate_from_frequencies(fd_grain)
plt.imshow(wc_grain, interpolation='bilinear')
plt.axis("off")
plt.show()

wc_trade = WordCloud(background_color="white").generate_from_frequencies(fd_trade)
plt.imshow(wc_trade, interpolation='bilinear')
plt.axis("off")
plt.show()

# Finally, how to generate TDM

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Create a matrix using term frequency first using CountVectorizer
# It requires the input to be text string
# The result is in sparse matrix format
vec_tf = CountVectorizer()
grain_tf = vec_tf.fit_transform(grain_raw)
grain_tf

# Where are the columns then?
vec_tf.get_feature_names()[1000:1050]

# index of a specific word
vec_tf.vocabulary_["grain"]

# you can set a minimum DF and maximum DF
vec_tf_2 = CountVectorizer(min_df = 2, max_df=0.7)
grain_tf_2 = vec_tf_2.fit_transform(grain_raw)
grain_tf_2

# and activate stopword removal
vec_tf_3 = CountVectorizer(min_df = 2, max_df=0.7, stop_words='english')
grain_tf_3 = vec_tf_3.fit_transform(grain_raw)
grain_tf_3

# To have binary indexing, set "binary=True"
vec_bin = CountVectorizer(binary=True, min_df = 2, max_df=0.7, stop_words='english')
grain_bin = vec_bin.fit_transform(grain_raw)
grain_bin.toarray()[:10, :20]

# And tfidf indexing
vec_tfidf = TfidfVectorizer(min_df = 2, max_df=0.7, stop_words='english')
grain_tfidf = vec_tfidf.fit_transform(grain_raw)
grain_tfidf
grain_tfidf.toarray()[:10, :20]

# Can we control the preprocessing more precisely? Yes.
# Recall that we've created the function myPrep()? We can use it with the vectorizer
myVec = TfidfVectorizer(tokenizer=myPrep, min_df = 2, max_df=0.7)
grain_v = myVec.fit_transform(grain_raw)
grain_v

# To save the vectorized results for future use
# You can find the files in the Colab session, which can be downloaded to 
# your local machine to keep.
import pickle
pickle.dump(grain_tfidf, open("grain-tfidf.pkl", "wb"))
pickle.dump(vec_tfidf.vocabulary_, open("grain-feature.pkl","wb"))

#load the content
loaded_vec = TfidfVectorizer(decode_error="replace",vocabulary=pickle.load(open("grain-feature.pkl", "rb")))
tfidf = pickle.load(open("grain-tfidf.pkl", "rb" ) )
tfidf
