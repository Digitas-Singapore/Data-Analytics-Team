# -*- coding: utf-8 -*-
"""DecisionTreeRandomForest28032022-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Digitas-Singapore/Data-Analytics-Team/blob/Shilton-Jonatan/.ipynb_checkpoints/DecisionTreeRandomForest28032022-checkpoint.ipynb

Owner:      Shilton Jonatan, Data & Analytics, Digitas Singapore, shilton.salindeho@digitas.com

Solution:   Decision Tree/Random Forest

Date of publication:  28 March 2022

## Decision Tree/Random Forest

Classification algorithms are used to categorize data into a class or category. Classification can be of three types: binary classification, multiclass classification, multilabel classification.

Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that the purity of the node increases with respect to the target variable.

The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.

<img src="https://github.com/Digitas-Singapore/Data-Analytics-Team/blob/Shilton-Jonatan/.ipynb_checkpoints/decision-tree-classification-algorithm.png?raw=1" width="350" />
<img src="https://github.com/Digitas-Singapore/Data-Analytics-Team/blob/Shilton-Jonatan/.ipynb_checkpoints/Random_forest_diagram_complete.png?raw=1" width="350" />
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split

"""Then, we load the sample data - in this sample, we'll be using a data of car brands and makes along with the specs."""

!curl --remote-name \
-H 'Accept: application/vnd.github.v3.raw' \
--location https://raw.githubusercontent.com/Digitas-Singapore/Data-Analytics-Team/Chilin-Tang/DatasetMtcars25032022.csv

# Load mtcars sample data set
mtcars = pd.read_csv("DatasetMtcars25032022.csv") #reads text data into data frame

#See first few lines of data
mtcars.head()

mtcars.shape

mtcars.vs.unique()

"""Decision Tree and Random Forest are both supervised algorithms - we will need input variables and output variables in order to train the model before the model can predict new sets of data.

In this case, we can try to split the data into training and test data, and define variable 'vs' as the output variable.
"""

#Set all variables except vs as X, and vs as y
X=mtcars.loc[:, ~mtcars.columns.isin(['vs', 'Unnamed: 0'])]
y=mtcars['vs']

X.head()

y

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

#Fitting the data into the tree
tree = DecisionTreeClassifier().fit(X_train, y_train)
randomforest = RandomForestClassifier().fit(X_train, y_train)

#Get prediction from fitted models
y_pred_tree = tree.predict(X_test)
y_pred_rf = randomforest.predict(X_test)

#Check for model accuracy scores
print("Decision Tree Accuracy:",metrics.accuracy_score(y_test, y_pred_tree))
print("Random Forest Accuracy:",metrics.accuracy_score(y_test, y_pred_rf))