# -*- coding: utf-8 -*-
"""MultipleLinearRegressionTraining25032022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Digitas-Singapore/Data-Analytics-Team/blob/Chilin-Tang/MultipleLinearRegressionTraining25032022.ipynb
"""

# Owner:      Chilin Tang, Data & Analytics, Digitas Singapore, chilin.tang@digitas.com

# Solution:   Multiple linear regression, predicting dependent variable (y) from one/more dependent variables (x) through mix modeling such as MMM

# Date of publication:  25 March 2022

# Data requirements:  
#                     1) Relationship between dependent variable and independent variables is (approximately) linear
#                     2) Expected mean error of the regression model is zero
#                     3) Residuals are uncorrelated with each others (i.e. no multicollinearity)
#                     4) Errors (residuals) are normally distributed and have 0 population mean
#                     5) Residuals do not vary with x (i.e. constant variance, no heteroskedacticity)

pip install scipy numpy pandas statsmodels pingouin

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
matplotlib.style.use('ggplot')
from sklearn import metrics
from sklearn import linear_model
from sklearn import datasets, linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
# import pingouin as pg
# from statsmodels.formula.api import ols

# Fetch a single <10MB file using the raw GitHub URL.
!curl --remote-name \
-H 'Accept: application/vnd.github.v3.raw' \
--location https://raw.githubusercontent.com/Digitas-Singapore/Data-Analytics-Team/Chilin-Tang/DatasetMtcars25032022.csv

# Load mtcars data set
mtcars = pd.read_csv("DatasetMtcars25032022.csv") #reads text data into data frame
# Open and read in data from osha.txt
# data_df  =  pandas.read_csv("osha.txt",delimiter="\t", header=None) #reads text data into data frame

#  Views the dataset of cars with diverse characteristics
mtcars

#  Structure of mtcars dataset
#	 name	Model of Vehicle	The car types are a mix that includes sedans (Datsun, Ford, Honda,…), luxury sedans (Mercedes, Cadellac,..), muscle cars (Javelin, Challenger, Camero…) and high-end sports cars (Porsche, Lotus, Maserati, Ferrari…)
#  mpg	Miles/US Gallon	mpg is the determinant of fuel efficiency
#  cyl	Number of cylinders	Data includes vehicles with 4,6,8 cylinder engines.
#  disp	Displacement (cu.in.)	Displacement measures overall volume in the engine as a factor of cylinder circumfrance, depth and total number of cylinders. This metric gives a good proxy for the total amount of power the engine can generate.
#  hp	Gross horsepower	Gross horsepower measures the theoretical output of an engine’s power output; notably, gross rating is of the engine in an isolated environment outside any specific vehicle. When installed in a car, exhaust systems, carburetor, alternator, power systems, etc all influence the power that actually gets to the drive train. Moreover, according to online sources, in the early 1970s, regulatory changes influenced how gross horsepower was measured. As this dataset is from the early-mid 1970s, it’s unclear if hp metrics may be used as reliable comparators of engine power across models as it’s uncertain how manufacturers are reporting.
#  drat	Rear axle ratio	The rear axle gear ratio indicates the number of turns of the drive shaft for every one rotation of the wheel axle. A vehicle with a high ratio would provide more torque and thus more towing capability, for example. Transmission configuration can often influence a manufacturer’s gearing ratio.
#  wt	Weight (lb/1000)	The overall weight of the vehicle per 1000lbs (half US ton)
#  qsec	1/4 mile time	A performance measure, primarily of acceleration. Fastest time to travel 1/4 mile from standstill (in seconds).
#  vs	V/S	Binary variable signaling the engine cylinder configuration a V-shape (vs=0) or Straight Line (vs=1). V==0 and S==1. Configuration offers trade offs in power/torque, design usage in terms of space/size of engine and performance or center of gravity of vehicle. The geometry and placement of the engine, as influenced by its cylinder head, can have numerous knock-on influences on the vehicle beyond the technical engineering considerations of the cyliner angle.
#	 am	Transmission Type	A binary variable signaling whether vehicle has automatic (am=0) or manual (am=1) transmission configuration.
#  gear	Number of forward gears	Number of gears in the transmission. Manual transmissions have either 4 or 5 forward gears; Automatic either 3 or 4
#  carb	Number of carburetors	The number of carburetor barrels. Engines with higher displacement typically have higher barrel configuration to accomodate the increased airflow rate of the larger engine; in other words, more capacity is available for an engine when it may need it versus constraining power output with limited barrels. A vehicle may have multiple physical carburetors, but it’s less common; this metric is the sum of the number of carburetors and the number of barrels inside the carburetor.

# Descriptive statistics for Exploratory Data Analysis
mtcars.describe()

# =================================== Developing the Multi Linear Regression Model ================================

# creates a model where mpg (miles per gallon, Y axis) is attributed from 10 other vehicle characteristics (X axis)
X=mtcars.iloc[:,2:]
Y=mtcars.mpg

X2 = sm.add_constant(X)
est = sm.OLS(Y, X2).fit()
print(est.summary2())

#  This is how linear regression table can be interpreted for attribution.  Statistically significant variables are: 
#  𝑚𝑝𝑔 = 9.6178 (residual) −3.9165∗𝑤𝑡 (weight) +1.2259∗𝑞𝑠𝑒𝑐 (fastest time to quarter mile)+2.9358∗𝑎𝑚 (transmission type)
#  Insights:  𝑚𝑝𝑔 is negatively dependent on the Weight (1000 lbs) of the car, 
#  positively correlated with 1/4 mile time and Transmission (0 = automatic, 1 = manual)

# =================== Evaluating goodness of fit and error mitigation (Multicollinearity and Variance Inflation Factor (VIF) ====================

#  When there is a high intercorrelation between two or more independent variables in a multiple regression model, it is called multicollinearity.
#  Multicollinearity can make the model unstable and the statistically significant feature may be labelled as statistically insignificant (large p-value).
#  Variance Inflation Factor (VIF) is measuring the magnitude of multicollinearity. VIF is given by  1/(1−𝑅²) . 
#  Its number indicates by how much larger the standard error is increased, in presence of multicollinearity. 
#  The threshold usually VIF>4 or VIF>10 to go through to check the impact of multicollinearity.

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif=[variance_inflation_factor(X.values, j) for j in range(X.shape[1])]
vif_factor=pd.DataFrame({'VIF': vif},index=X.columns)
vif_factor

#  Checking the correlation matrix

sns.set_style("whitegrid")
mask = np.triu(np.ones_like(X.corr()))
corr = sns.heatmap(X.corr(), annot=True, cmap='coolwarm', mask=mask)

#  Feature selection:  The correlation matrix above shown that there are high correlation ( >0.70 ) between features:
#  (cyl: vs, wt, drat, hp, disp)
#  (hp: carb, vs, qsec)
#  (drat: gear, am, wt)
#  We have to choose just one feature from each group to prevent multicollinearity. If we combine the method with p-value and consider VIF.

vif_factor['p-values']=est.pvalues[1:]
vif_factor

#  Measured between VIF and p-values, the feature sets least impacted by multicollinearity are:

#  (cyl: vs, wt, drat, hp, disp) -> wt (the lowest p-value)
#  (hp: carb, vs, qsec) -> qsec (the lowest p-value)
#  (drat: gear, am, wt) -> am, wt (even if drat-am and drat-wt are highly correlated, but am-wt is not)

# Rebuild Multiple Linear Regression model with independent variables least affected by multicollinearity wt, qsec, am 
X3 = X2[['const','wt','qsec','am']]
est2 = sm.OLS(Y, X3).fit()
print(est2.summary2())

#  This results in a better model, with Adjusted R-squared being higher at 83.4% (compared to 80.7%). 
#  Multicollinearity is already handled and all the variables are statistically significant (p-value < 0.05).

#  Other ways to handling multicollinearity are:

#  1.  Use centred variables for example use  𝑋𝑖−𝑋 mean  instead of  𝑋𝑖 , or any other transformation
#  2.  Apply Principal Component Analysis (PCA) to reduce features to a smaller set of uncorrelated (orthogonal) components.

# =================================== Residual Analysis (to see if normally distributed) ============================================

# Test for Normal Distribution of Residuals
probplot = sm.ProbPlot(est2.resid)
plt.figure()
probplot.ppplot(line='45')
plt.show()

# =================================== Homoscedasticity Test (Residuals do not vary with x) ============================================

def standardized_values(vals):
    return (vals - vals.mean())/vals.std()
plt.scatter(est2.fittedvalues, est2.resid)
plt.xlabel("Standardized Predicted Values")
plt.ylabel("Standardized Residual Values")
plt.show()

# The residuals are very random and do not follow any pattern. This means the residuals have constant variance (homoscedasticity)

# =================================== Checks for outliers in independent (x) values  ============================================

mlr_inf = est2.get_influence()
(c, p) = mlr_inf.cooks_distance
plt.stem(np.arange(len(X)), np.round(c, 3), markerfmt=',')
plt.show()

#  There is no outlier in this dataset from final model because Cook’s distance for all the data points is less than 1.